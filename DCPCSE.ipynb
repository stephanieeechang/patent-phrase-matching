{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCPCSE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Phrase-to-Phrase Matching Using [DCPCSE](https://github.com/YJiangcm/DCPCSE)\n",
        "\n",
        "Created for GaTech CS7650 Final Project"
      ],
      "metadata": {
        "id": "WE_7kKy3jKA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the Dataset\n",
        "\n",
        "Steps to get this working:\n",
        "1. Go to your Kaggle Account, and get a \"New API Token\" which installs a json file.\n",
        "2. Upload this file into Colab under root/.kaggle (need to toggle visibility of hidden directories to see this)"
      ],
      "metadata": {
        "id": "qT8ITQr9m2jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 /root/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c us-patent-phrase-to-phrase-matching --force\n",
        "! unzip -q us-patent-phrase-to-phrase-matching.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zugFVRhTm1kE",
        "outputId": "d3b9becb-56ed-4617-b107-569ed846fbac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading us-patent-phrase-to-phrase-matching.zip to /content/gdrive/MyDrive/7650_DCPCSE/DCPCSE\n",
            "\r  0% 0.00/682k [00:00<?, ?B/s]\n",
            "\r100% 682k/682k [00:00<00:00, 40.1MB/s]\n",
            "replace sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloning DCPCSE"
      ],
      "metadata": {
        "id": "YWEQBeqSnRgG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6lddH6pjAMk",
        "outputId": "7b1a70e0-17d7-40fa-89be-8c4c64ffd07b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ~/../content\n",
        "! mkdir gdrive/MyDrive/7650_DCPCSE\n",
        "! mkdir results\n",
        "%cd gdrive/MyDrive/7650_DCPCSE\n",
        "! git clone https://github.com/YJiangcm/DCPCSE.git\n",
        "%cd DCPCSE\n",
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1_cGC0Fcki3j",
        "outputId": "51bab61d-9f8e-4a3e-8880-4352dd4558b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "mkdir: cannot create directory ‘gdrive/MyDrive/7650_DCPCSE’: File exists\n",
            "/content/gdrive/MyDrive/7650_DCPCSE\n",
            "fatal: destination path 'DCPCSE' already exists and is not an empty directory.\n",
            "/content/gdrive/MyDrive/7650_DCPCSE/DCPCSE\n",
            "Collecting transformers==4.2.1\n",
            "  Downloading transformers-4.2.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 29.2 MB/s \n",
            "\u001b[?25hCollecting scipy==1.5.4\n",
            "  Downloading scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 11.8 MB/s \n",
            "\u001b[?25hCollecting datasets==1.2.1\n",
            "  Downloading datasets-1.2.1-py3-none-any.whl (159 kB)\n",
            "\u001b[K     |████████████████████████████████| 159 kB 64.9 MB/s \n",
            "\u001b[?25hCollecting pandas==1.1.5\n",
            "  Downloading pandas-1.1.5-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5 MB 53.1 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.24.0\n",
            "  Downloading scikit_learn-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 66.8 MB/s \n",
            "\u001b[?25hCollecting prettytable==2.1.0\n",
            "  Downloading prettytable-2.1.0-py3-none-any.whl (22 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-2.9.4-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.11.0+cu113)\n",
            "Collecting setuptools==49.3.0\n",
            "  Downloading setuptools-49.3.0-py3-none-any.whl (790 kB)\n",
            "\u001b[K     |████████████████████████████████| 790 kB 52.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r requirements.txt (line 1)) (4.11.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r requirements.txt (line 1)) (1.21.6)\n",
            "Collecting tokenizers==0.9.4\n",
            "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 58.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 53.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r requirements.txt (line 1)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.2.1->-r requirements.txt (line 1)) (4.64.0)\n",
            "Collecting tqdm>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->-r requirements.txt (line 3)) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->-r requirements.txt (line 3)) (0.70.12.2)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 77.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.2.1->-r requirements.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirements.txt (line 4)) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.0->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24.0->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable==2.1.0->-r requirements.txt (line 6)) (0.2.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1->-r requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1->-r requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1->-r requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.2.1->-r requirements.txt (line 1)) (2.10)\n",
            "Collecting analytics-python\n",
            "  Downloading analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.17.6-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 60.9 MB/s \n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.75.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.6.8-cp37-cp37m-manylinux_2_24_x86_64.whl (253 kB)\n",
            "\u001b[K     |████████████████████████████████| 253 kB 76.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 55.5 MB/s \n",
            "\u001b[?25hCollecting paramiko\n",
            "  Downloading paramiko-2.10.4-py2.py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 72.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from gradio->-r requirements.txt (line 7)) (2.11.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio->-r requirements.txt (line 7)) (3.2.2)\n",
            "Collecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio->-r requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r requirements.txt (line 8)) (4.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio->-r requirements.txt (line 7)) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio->-r requirements.txt (line 7)) (21.4.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 78.0 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 80.7 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting backoff==1.10.0\n",
            "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting monotonic>=1.5\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting starlette==0.17.1\n",
            "  Downloading starlette-0.17.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2\n",
            "  Downloading pydantic-1.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 76.2 MB/s \n",
            "\u001b[?25hCollecting anyio<4,>=3.0.0\n",
            "  Downloading anyio-3.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.2.1->-r requirements.txt (line 1)) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->gradio->-r requirements.txt (line 7)) (2.0.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.1-py3-none-any.whl (10 kB)\n",
            "Collecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio->-r requirements.txt (line 7)) (1.4.2)\n",
            "Collecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[K     |████████████████████████████████| 856 kB 58.1 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "  Downloading cryptography-37.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-3.2.2-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko->gradio->-r requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio->-r requirements.txt (line 7)) (2.21)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.2.1->-r requirements.txt (line 1)) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.6 MB/s \n",
            "\u001b[?25hCollecting asgiref>=3.4.0\n",
            "  Downloading asgiref-3.5.1-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: ffmpy, python-multipart\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=c51146e58975a35705b0d9bb6739e12c16ad8a4235ddb39388e0f5255cb354d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=873f888ef03f79d87bc33c9dca246f5db5688bf6623b0fa40948c93d4affc7a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "Successfully built ffmpy python-multipart\n",
            "Installing collected packages: sniffio, mdurl, uc-micro-py, multidict, markdown-it-py, frozenlist, anyio, yarl, tqdm, starlette, pynacl, pydantic, monotonic, mdit-py-plugins, linkify-it-py, h11, cryptography, bcrypt, backoff, asynctest, async-timeout, asgiref, aiosignal, xxhash, uvicorn, tokenizers, scipy, sacremoses, python-multipart, pydub, pycryptodome, paramiko, pandas, orjson, ffmpy, fastapi, analytics-python, aiohttp, transformers, setuptools, scikit-learn, prettytable, gradio, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 3.2.0\n",
            "    Uninstalling prettytable-3.2.0:\n",
            "      Successfully uninstalled prettytable-3.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 analytics-python-1.4.0 anyio-3.5.0 asgiref-3.5.1 async-timeout-4.0.2 asynctest-0.13.0 backoff-1.10.0 bcrypt-3.2.2 cryptography-37.0.1 datasets-1.2.1 fastapi-0.75.2 ffmpy-0.3.0 frozenlist-1.3.0 gradio-2.9.4 h11-0.13.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.0 mdurl-0.1.1 monotonic-1.6 multidict-6.0.2 orjson-3.6.8 pandas-1.1.5 paramiko-2.10.4 prettytable-2.1.0 pycryptodome-3.14.1 pydantic-1.9.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 sacremoses-0.0.49 scikit-learn-0.24.0 scipy-1.5.4 setuptools-49.3.0 sniffio-1.2.0 starlette-0.17.1 tokenizers-0.9.4 tqdm-4.49.0 transformers-4.2.1 uc-micro-py-1.0.1 uvicorn-0.17.6 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "! ls ../../../../"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FlMwJQClMlq",
        "outputId": "7d98ae96-cae4-452e-eed3-7f3e1000b679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gdrive\tresults  sample_data  sample_submission.csv  test.csv  train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('../../../../train.csv')\n",
        "# test_df = pd.read_csv('../../../../test.csv')\n",
        "train, test = train_test_split(train_df, shuffle=True, test_size=0.20, random_state=17)\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "\n",
        "SCORE_CUTOFF = 0.25\n",
        "\n",
        "train_all = train.drop(['id', 'context'], axis=1)\n",
        "# test = test.drop(['id', 'context'], axis=1)\n",
        "\n",
        "train_pos = train_all[train_all['score'] >= SCORE_CUTOFF].drop('score', axis=1)\n",
        "train_neg = train_all[train_all['score'] < SCORE_CUTOFF].drop('score', axis=1)\n",
        "\n",
        "sentences = train_pos.merge(train_neg, how='inner', on='anchor', suffixes=['_pos', '_neg'])\n",
        "sentences = sentences.drop_duplicates(subset=['anchor', 'target_pos'])\n",
        "\n",
        "print(sentences.head())\n",
        "print(sentences.shape)\n",
        "\n",
        "\n",
        "\n",
        "sentences.to_csv('../../../../train_sentences.csv', index=False)\n",
        "train.to_csv('SentEval/data/downstream/STS/STSBenchmark/train_sentences.csv', index=False)\n",
        "test.to_csv('SentEval/data/downstream/STS/STSBenchmark/test_sentences.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbmTs60F4KXo",
        "outputId": "8f50c3c3-a7e6-401f-d664-74f4b7b7511d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(29178, 5)\n",
            "(7295, 5)\n",
            "                        anchor                 target_pos         target_neg\n",
            "0   perform working operations            perform working  working principle\n",
            "10  perform working operations     perform working action  working principle\n",
            "20  perform working operations   metal working operations  working principle\n",
            "30  perform working operations  perform working operation  working principle\n",
            "40  perform working operations  execute working operation  working principle\n",
            "(21413, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py \\\n",
        "  --model_name_or_path roberta-large \\\n",
        "  --train_file ../../../../train_sentences.csv \\\n",
        "  --output_dir ../../../../results \\\n",
        "  --num_train_epochs 10 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --learning_rate 5e-3 \\\n",
        "  --max_seq_length 32 \\\n",
        "  --metric_for_best_model stsb_spearman \\\n",
        "  --load_best_model_at_end \\\n",
        "  --pooler_type cls \\\n",
        "  --pre_seq_len 10 \\\n",
        "  --overwrite_output_dir \\\n",
        "  --eval_steps 100 \\\n",
        "  --temp 0.05 \\\n",
        "  --do_train \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwHbj7F8Jqpw",
        "outputId": "f224565e-29a1-40e6-9e9b-326fe3253e9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "05/02/2022 02:32:25 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1 distributed training: False, 16-bits training: False\n",
            "05/02/2022 02:32:25 - INFO - __main__ -   Training/evaluation parameters OurTrainingArguments(output_dir='../../../../results', overwrite_output_dir=True, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=64, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.005, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=10.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_steps=0, logging_dir='runs/May02_02-32-25_1e3fc9ec83a0', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=100, dataloader_num_workers=0, past_index=-1, run_name='../../../../results', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='stsb_spearman', greater_is_better=True, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, eval_transfer=False)\n",
            "Using custom data configuration default\n",
            "Downloading and preparing dataset csv/default-39afc54b9476d6e6 (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to ./data/csv/default-39afc54b9476d6e6/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2...\n",
            "Dataset csv downloaded and prepared to ./data/csv/default-39afc54b9476d6e6/0.0.0/2960f95a26e85d40ca41a230ac88787f715ee3003edaacb8b1f0891e9f04dda2. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:445] 2022-05-02 02:32:25,976 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:481] 2022-05-02 02:32:25,976 >> Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:445] 2022-05-02 02:32:26,003 >> loading configuration file https://huggingface.co/roberta-large/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/dea67b44b38d504f2523f3ddb6acb601b23d67bee52c942da336fa1283100990.94cae8b3a8dbab1d59b9d4827f7ce79e73124efa6bb970412cd503383a95f373\n",
            "[INFO|configuration_utils.py:481] 2022-05-02 02:32:26,003 >> Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.2.1\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1766] 2022-05-02 02:32:26,088 >> loading file https://huggingface.co/roberta-large/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/7c1ba2435b05451bc3b4da073c8dec9630b22024a65f6c41053caccf2880eb8f.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
            "[INFO|tokenization_utils_base.py:1766] 2022-05-02 02:32:26,088 >> loading file https://huggingface.co/roberta-large/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/20b5a00a80e27ae9accbe25672aba42ad2d4d4cb2c4b9359b50ca8e34e107d6d.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1766] 2022-05-02 02:32:26,089 >> loading file https://huggingface.co/roberta-large/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/e16a2590deb9e6d73711d6e05bf27d832fa8c1162d807222e043ca650a556964.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
            "[INFO|modeling_utils.py:1027] 2022-05-02 02:32:26,187 >> loading weights file https://huggingface.co/roberta-large/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e36ec2f5052bec1e79e139b84c2c3089cb647694ba0f4f634fec7b8258f7c89.c43841d8c5cd23c435408295164cda9525270aa42cd0cc9200911570c0342352\n",
            "total param is 354801664, trainable param is 491520\n",
            "[WARNING|modeling_utils.py:1135] 2022-05-02 02:32:35,210 >> Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForCL: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForCL from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForCL from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1146] 2022-05-02 02:32:35,210 >> Some weights of RobertaForCL were not initialized from the model checkpoint at roberta-large and are newly initialized: ['prefix_encoder.embedding.weight', 'mlp.dense.weight', 'mlp.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 22/22 [00:01<00:00, 11.55ba/s]\n",
            "[INFO|trainer.py:442] 2022-05-02 02:32:42,460 >> The following columns in the training set don't have a corresponding argument in `RobertaForCL.forward` and have been ignored: .\n",
            "05/02/2022 02:32:42 - INFO - dcpcse.trainers -   ***** Running training *****\n",
            "05/02/2022 02:32:42 - INFO - dcpcse.trainers -     Num examples = 21413\n",
            "05/02/2022 02:32:42 - INFO - dcpcse.trainers -     Num Epochs = 10\n",
            "05/02/2022 02:32:42 - INFO - dcpcse.trainers -     Instantaneous batch size per device = 64\n",
            "05/02/2022 02:32:42 - INFO - dcpcse.trainers -     Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "05/02/2022 02:32:42 - INFO - dcpcse.trainers -     Gradient Accumulation steps = 1\n",
            "05/02/2022 02:32:42 - INFO - dcpcse.trainers -     Total optimization steps = 3350\n",
            "{'loss': 3.8261, 'learning_rate': 0.004253731343283582, 'epoch': 1.49}\n",
            "{'loss': 2.3021, 'learning_rate': 0.0035074626865671645, 'epoch': 2.99}\n",
            "{'loss': 1.9665, 'learning_rate': 0.0027611940298507467, 'epoch': 4.48}\n",
            "{'loss': 1.8043, 'learning_rate': 0.0020149253731343284, 'epoch': 5.97}\n",
            "{'loss': 1.6729, 'learning_rate': 0.0012686567164179106, 'epoch': 7.46}\n",
            "{'loss': 1.5843, 'learning_rate': 0.0005223880597014925, 'epoch': 8.96}\n",
            "100% 3350/3350 [40:58<00:00,  1.27it/s]05/02/2022 03:13:40 - INFO - dcpcse.trainers -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2458.4233, 'train_samples_per_second': 1.363, 'epoch': 10.0}\n",
            "100% 3350/3350 [40:58<00:00,  1.36it/s]\n",
            "[INFO|trainer.py:1344] 2022-05-02 03:13:41,351 >> Saving model checkpoint to ../../../../results\n",
            "[INFO|configuration_utils.py:300] 2022-05-02 03:13:41,354 >> Configuration saved in ../../../../results/config.json\n",
            "[INFO|modeling_utils.py:817] 2022-05-02 03:13:46,704 >> Model weights saved in ../../../../results/pytorch_model.bin\n",
            "05/02/2022 03:13:46 - INFO - __main__ -   ***** Train results *****\n",
            "05/02/2022 03:13:46 - INFO - __main__ -     epoch = 10.0\n",
            "05/02/2022 03:13:46 - INFO - __main__ -     train_runtime = 2458.4233\n",
            "05/02/2022 03:13:46 - INFO - __main__ -     train_samples_per_second = 1.363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the results from running DCPSCE"
      ],
      "metadata": {
        "id": "VD2cJeruqZr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/dcpcse.zip /content/results\n",
        "\n",
        "files.download('/content/dcpcse.zip')"
      ],
      "metadata": {
        "id": "zYZdySYcqGYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on Test data"
      ],
      "metadata": {
        "id": "W9Vxz9oFtgzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, download evaluation datasets"
      ],
      "metadata": {
        "id": "M49gE3UnuOtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd SentEval/data/downstream/\n",
        "! bash download_dataset.sh\n",
        "%cd ../../.."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TQe9axYtkEq",
        "outputId": "5bc1742c-ccad-4c6f-9b47-312e095ce0eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/7650_DCPCSE/DCPCSE/SentEval/data/downstream\n",
            "--2022-05-01 23:43:01--  https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/senteval.tar\n",
            "Resolving huggingface.co (huggingface.co)... 34.225.34.242, 34.197.58.156, 54.161.5.137, ...\n",
            "Connecting to huggingface.co (huggingface.co)|34.225.34.242|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/datasets/princeton-nlp/datasets-for-simcse/bc43c148f7be97471c78fc4255399d3158cb99dfe8f2221999c918338b138c38 [following]\n",
            "--2022-05-01 23:43:01--  https://cdn-lfs.huggingface.co/datasets/princeton-nlp/datasets-for-simcse/bc43c148f7be97471c78fc4255399d3158cb99dfe8f2221999c918338b138c38\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 52.85.130.79, 52.85.130.5, 52.85.130.28, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|52.85.130.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 89825280 (86M) [application/octet-stream]\n",
            "Saving to: ‘senteval.tar.4’\n",
            "\n",
            "senteval.tar.4      100%[===================>]  85.66M  59.9MB/s    in 1.4s    \n",
            "\n",
            "2022-05-01 23:43:02 (59.9 MB/s) - ‘senteval.tar.4’ saved [89825280/89825280]\n",
            "\n",
            "CR/\n",
            "CR/custrev.neg\n",
            "CR/custrev.pos\n",
            "MPQA/\n",
            "MPQA/mpqa.neg\n",
            "MPQA/mpqa.pos\n",
            "MR/\n",
            "MR/rt-polarity.neg\n",
            "MR/rt-polarity.pos\n",
            "MRPC/\n",
            "MRPC/msr_paraphrase_train.txt\n",
            "MRPC/msr_paraphrase_test.txt\n",
            "SICK/\n",
            "SICK/SICK_trial.txt\n",
            "SICK/SICK_train.txt\n",
            "SICK/SICK_test_annotated.txt\n",
            "SNLI/\n",
            "SNLI/s2.test\n",
            "SNLI/s1.train\n",
            "SNLI/s2.train\n",
            "SNLI/labels.dev\n",
            "SNLI/s1.test\n",
            "SNLI/labels.test\n",
            "SNLI/s2.dev\n",
            "SNLI/s1.dev\n",
            "SNLI/labels.train\n",
            "SST/\n",
            "SST/fine/\n",
            "SST/fine/sentiment-test\n",
            "SST/fine/sentiment-train\n",
            "SST/fine/sentiment-dev\n",
            "SST/binary/\n",
            "SST/binary/sentiment-test\n",
            "SST/binary/sentiment-train\n",
            "SST/binary/sentiment-dev\n",
            "STS/\n",
            "STS/STS12-en-test/\n",
            "STS/STS12-en-test/STS.gs.surprise.OnWN.txt\n",
            "STS/STS12-en-test/STS.input.surprise.OnWN.txt\n",
            "STS/STS12-en-test/STS.input.MSRpar.txt\n",
            "STS/STS12-en-test/STS.gs.ALL.txt\n",
            "STS/STS12-en-test/00-readme.txt\n",
            "STS/STS12-en-test/STS.gs.MSRvid.txt\n",
            "STS/STS12-en-test/STS.input.MSRvid.txt\n",
            "STS/STS12-en-test/STS.gs.MSRpar.txt\n",
            "STS/STS12-en-test/STS.input.surprise.SMTnews.txt\n",
            "STS/STS12-en-test/STS.gs.SMTeuroparl.txt\n",
            "STS/STS12-en-test/STS.gs.surprise.SMTnews.txt\n",
            "STS/STS12-en-test/STS.input.SMTeuroparl.txt\n",
            "STS/STS14-en-test/\n",
            "STS/STS14-en-test/STS.input.headlines.txt\n",
            "STS/STS14-en-test/STS.gs.OnWN.txt\n",
            "STS/STS14-en-test/STS.gs.images.txt\n",
            "STS/STS14-en-test/STS.gs.deft-news.txt\n",
            "STS/STS14-en-test/STS.gs.tweet-news.txt\n",
            "STS/STS14-en-test/sts2012-train.tgz\n",
            "STS/STS14-en-test/sts2013-test.tgz\n",
            "STS/STS14-en-test/00-readme.txt\n",
            "STS/STS14-en-test/STS.input.OnWN.txt\n",
            "STS/STS14-en-test/STS.input.deft-news.txt\n",
            "STS/STS14-en-test/sts2012-test.tgz\n",
            "STS/STS14-en-test/STS.input.deft-forum.txt\n",
            "STS/STS14-en-test/STS.output.headlines.txt\n",
            "STS/STS14-en-test/correlation-noconfidence.pl\n",
            "STS/STS14-en-test/STS.gs.headlines.txt\n",
            "STS/STS14-en-test/STS.gs.deft-forum.txt\n",
            "STS/STS14-en-test/STS.input.tweet-news.txt\n",
            "STS/STS14-en-test/STS.input.images.txt\n",
            "STS/STS15-en-test/\n",
            "STS/STS15-en-test/STS.input.headlines.txt\n",
            "STS/STS15-en-test/STS.gs.images.txt\n",
            "STS/STS15-en-test/STS.gs.answers-students.txt\n",
            "STS/STS15-en-test/00-readme.txt\n",
            "STS/STS15-en-test/STS.input.answers-students.txt\n",
            "STS/STS15-en-test/STS.input.answers-forums.LICENSE\n",
            "STS/STS15-en-test/STS.gs.answers-forums.txt\n",
            "STS/STS15-en-test/STS.input.answers-forums.txt\n",
            "STS/STS15-en-test/STS.gs.belief.txt\n",
            "STS/STS15-en-test/correlation-noconfidence.pl\n",
            "STS/STS15-en-test/STS.input.belief.txt\n",
            "STS/STS15-en-test/STS.gs.headlines.txt\n",
            "STS/STS15-en-test/STS.answers-forums.zip\n",
            "STS/STS15-en-test/corebaseline-tokencos.tar.gz\n",
            "STS/STS15-en-test/STS.input.images.txt\n",
            "STS/STS13-en-test/\n",
            "STS/STS13-en-test/STS.input.headlines.txt\n",
            "STS/STS13-en-test/STS.gs.OnWN.txt\n",
            "STS/STS13-en-test/correlation.pl\n",
            "STS/STS13-en-test/STS.input.FNWN.txt\n",
            "STS/STS13-en-test/STS.gs.FNWN.txt\n",
            "STS/STS13-en-test/STS.output.FNWN.txt\n",
            "STS/STS13-en-test/00-readme.txt\n",
            "STS/STS13-en-test/correlation-all.pl\n",
            "STS/STS13-en-test/STS.input.OnWN.txt\n",
            "STS/STS13-en-test/STS.output.headlines.txt\n",
            "STS/STS13-en-test/correct-output.pl\n",
            "STS/STS13-en-test/STS.gs.headlines.txt\n",
            "STS/STS13-en-test/STS.output.SMT.txt\n",
            "STS/STS13-en-test/STS.gs.SMT.txt\n",
            "STS/STS13-en-test/STS.output.OnWN.txt\n",
            "STS/STSBenchmark/\n",
            "STS/STSBenchmark/correlation.pl\n",
            "STS/STSBenchmark/sts-test.csv\n",
            "STS/STSBenchmark/readme.txt\n",
            "STS/STSBenchmark/LICENSE.txt\n",
            "STS/STSBenchmark/sts-train.csv\n",
            "STS/STSBenchmark/sts-dev.csv\n",
            "STS/STS16-en-test/\n",
            "STS/STS16-en-test/STS.input.headlines.txt\n",
            "STS/STS16-en-test/STS.gs.plagiarism.txt\n",
            "STS/STS16-en-test/STS.gs.question-question.txt\n",
            "STS/STS16-en-test/STS.input.question-question.txt\n",
            "STS/STS16-en-test/STS2016.input.headlines.ascii\n",
            "STS/STS16-en-test/README.txt\n",
            "STS/STS16-en-test/STS.gs.answer-answer.txt\n",
            "STS/STS16-en-test/STS2016.input.question-question.ascii\n",
            "STS/STS16-en-test/STS.input.postediting.txt\n",
            "STS/STS16-en-test/STS.input.plagiarism.txt\n",
            "STS/STS16-en-test/STS.gs.postediting.txt\n",
            "STS/STS16-en-test/LICENSE.txt\n",
            "STS/STS16-en-test/STS.input.answer-answer.txt\n",
            "STS/STS16-en-test/correlation-noconfidence.pl\n",
            "STS/STS16-en-test/STS.gs.headlines.txt\n",
            "STS/STS16-en-test/STS2016.input.answer-answer.ascii\n",
            "STS/STS16-en-test/STS2016.input.postediting.ascii\n",
            "STS/STS16-en-test/STS2016.input.plagiarism.ascii\n",
            "SUBJ/\n",
            "SUBJ/subj.subjective\n",
            "SUBJ/subj.objective\n",
            "TREC/\n",
            "TREC/train_5500.label\n",
            "TREC/TREC_10.label\n",
            "/content/gdrive/MyDrive/7650_DCPCSE/DCPCSE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python evaluation.py \\\n",
        "    --model_name_or_path ../../../../results/ \\\n",
        "    --pooler_type cls \\\n",
        "    --task_set na \\\n",
        "    --tasks STSBenchmark \\\n",
        "    --mode test "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT8VqOkFuSxv",
        "outputId": "fcfcb305-46a5-4c49-f5e7-82276275b473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total param is 354801664, trainable param is 491520\n",
            "2022-05-02 03:14:02,215 : \n",
            "\n",
            "***** Transfer task : STSBenchmark*****\n",
            "\n",
            "\n",
            "2022-05-02 03:15:51,584 : train : pearson = 0.7110, spearman = 0.7113\n",
            "2022-05-02 03:16:19,037 : test : pearson = 0.6764, spearman = 0.6738\n",
            "2022-05-02 03:16:19,059 : ALL : Pearson = 0.7035,             Spearman = 0.7034\n",
            "2022-05-02 03:16:19,059 : ALL (weighted average) : Pearson = 0.7041,             Spearman = 0.7038\n",
            "2022-05-02 03:16:19,059 : ALL (average) : Pearson = 0.6937,             Spearman = 0.6925\n",
            "\n",
            "------ test ------\n",
            "+-------+-------+-------+-------+-------+--------------+-----------------+------+\n",
            "| STS12 | STS13 | STS14 | STS15 | STS16 | STSBenchmark | SICKRelatedness | Avg. |\n",
            "+-------+-------+-------+-------+-------+--------------+-----------------+------+\n",
            "|  0.00 |  0.00 |  0.00 |  0.00 |  0.00 |    67.38     |       0.00      | 9.63 |\n",
            "+-------+-------+-------+-------+-------+--------------+-----------------+------+\n",
            "+------+------+------+------+------+------+------+------+\n",
            "|  MR  |  CR  | SUBJ | MPQA | SST2 | TREC | MRPC | Avg. |\n",
            "+------+------+------+------+------+------+------+------+\n",
            "| 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 | 0.00 |\n",
            "+------+------+------+------+------+------+------+------+\n"
          ]
        }
      ]
    }
  ]
}